from igp2.opendrive.map import Map
import numpy as np
import random
import math
import logging
import copy
from typing import Callable, List, Dict, Tuple

from igp2.agent import AgentState, TrajectoryAgent
from igp2.cost import Cost
from igp2.goal import Goal, PointGoal
from igp2.trajectory import *
from igp2.velocitysmoother import VelocitySmoother
from igp2.planlibrary.maneuver import Maneuver
from igp2.recognition.astar import AStar

logger = logging.getLogger(__name__)

class GoalWithType:
    """Tuple of a Goal object and a goal_type string defining an action the vehicle has to perform to
     reach the goal (e.g. turn left). In the current implementation, goal_types are not defined."""
    def __init__(self):
        print ("never called in this case")

    def __new__(cls, goal : Goal = None, goal_type : str = None):
        return (goal, goal_type)

class GoalsProbabilities:
    """Class used to store and update goalprobabilities, as well as store useful results such as the priors,
     likelihoods, generated trajectories and rewards """
    def __init__(self, goals : List[Goal] = None, goal_types : List[List[str]] = None, priors : List[float] = None):
        """Creates a new GoalsProbabilities object.
        
        Args:
            goals: a list of goals objects representing the scenarios goals.
            goal_types: optionally, refine the goals with goal types.
            priors: optionally, a list of goal priors measured from the dataset. 
                    If unused, the priors will be set to a uniform distribution.
        """
        self._goals_and_types = []
        if goal_types is None:
            for goal in goals:
                goal_and_type = GoalWithType(goal, None)
                self._goals_and_types.append(goal_and_type)
        else:
            for goal, goal_type_arr in zip(goals, goal_types):
                for goal_type in goal_type_arr:
                    goal_and_type = GoalWithType(goal, goal_type)
                    self._goals_and_types.append(goal_and_type)

        if priors == None:
            self._goals_priors = dict.fromkeys(self._goals_and_types, self.uniform_distribution())
        else:
            self._goals_priors = dict(zip(self._goals_and_types, priors))
        self._goals_probabilities = copy.copy(self._goals_priors)
        self._optimum_trajectory = dict.fromkeys(self._goals_and_types, None)
        self._current_trajectory = copy.copy(self._optimum_trajectory)
        self._optimum_reward = copy.copy(self._optimum_trajectory)
        self._current_reward = copy.copy(self._optimum_trajectory)
        self._reward_difference = copy.copy(self._optimum_trajectory)
        self._likelihood = copy.copy(self._optimum_trajectory)

    def uniform_distribution(self) -> float:
        """Generates a uniform distribution across each GoalWithType object"""
        return float(1/len(self._goals_and_types))

    def sample(self, k : int = 1) -> List[GoalWithType]:
        """Used to randomly sample a goal according to the goals probability distribution."""
        return random.choices(list(self.goals_probabilities.keys()), weights=self.goals_probabilities.values(), k=k)

    @property
    def goals_probabilities(self) -> Dict[GoalWithType, float]:
        """Returns the current goals probabilities."""
        return self._goals_probabilities

    @property
    def goals_priors(self) -> Dict[GoalWithType, float]:
        """Return the goals priors."""
        return self._goals_priors

    @property
    def optimum_trajectory(self) -> Dict[GoalWithType, VelocityTrajectory]:
        """Returns the trajectory from initial vehicle position generated to each goal to calculate the likelihood."""
        return self._optimum_trajectory

    @property
    def current_trajectory(self) -> Dict[GoalWithType, VelocityTrajectory]:
        """Returns the real vehicle trajectory, extended by the trajectory
         from current vehicle position that was generated to each goal to calculate the likelihood."""
        return self._current_trajectory

    @property
    def optimum_reward(self) -> Dict[GoalWithType, float]:
        """Returns the reward generated by the optimum_trajectory for each goal, if we are not using the reward_as_difference toggle."""
        return self._optimum_reward

    @property
    def current_reward(self) -> Dict[GoalWithType, float]:
        """Returns the reward generated by the current_trajectory for each goal, if we are not using the reward_as_difference toggle."""
        return self._current_reward

    @property
    def reward_difference(self) -> Dict[GoalWithType, float]:
        """Returns the reward generated by the optimum_trajectory for each goal, if we are not using the reward_as_difference toggle."""
        return self._reward_difference

    @property
    def likelihood(self) -> Dict[GoalWithType, float]:
        """Returns the computed likelihoods for each goal"""
        return self._likelihood

class GoalRecognition:
    """This class updates exising goal probabilities using likelihoods computed from the vehicle current trajectory."""

    def __init__(self, astar: AStar, smoother: VelocitySmoother, scenario_map: Map, cost: Cost = None,
                beta: float = 1., reward_as_difference: bool = True):
        """Initialises a goal recognition class that will be used to update a GoalProbabilities object.
        
        Args:
            astar: Astar object used to generate trajectories
            smoother: Velocity smoother object used to make the astar generated trajectories realistic
            scenario_map: a Map object representing the current scenario
            cost: a Cost object representing how the reward associated to each trajectory will be computed.
            beta: scaling parameter for the Boltzmann distribution generating the likelihoods
            reward_as_difference: choose if we define the reward for each trajectory separately or if the
                                  reward is computed from differences of the different trajectory 
                                  quantities alongside the pathlength.
        """
        self._beta = beta
        self._reward_as_difference = reward_as_difference
        self._astar = astar
        self._smoother = smoother
        self._cost = Cost() if cost is None else cost
        self._scenario_map = scenario_map

    def update_goals_probabilities(self, goals_probabilities: GoalsProbabilities, 
            trajectory: StateTrajectory, agentId: int, frame_ini: Dict[int, AgentState], 
            frame: Dict[int, AgentState], maneuver: Maneuver = None) -> GoalsProbabilities :
        """Updates the goal probabilities, and stores relevant information in the GoalsProbabilities object.
        
        Args: 
            goals_probabilities: GoalsProbabilities object to update
            trajectory: current vehicle trajectory
            agentID: id of agent in current frame
            frame_ini: frame corresponding to the first state of the agent's trajectory
            frame: current frame
            maneuver: current maneuver in execution by the agent
        """
        norm_factor = 0.
        for goal_and_type, prob in goals_probabilities.goals_probabilities.items():
            try:
                goal = goal_and_type[0]
                #4. and 5. Generate optimum trajectory from initial point and smooth it
                if goals_probabilities.optimum_trajectory[goal_and_type] == None:
                    logger.debug("Generating optimum trajectory")
                    goals_probabilities.optimum_trajectory[goal_and_type] = \
                        self._generate_trajectory(agentId, frame_ini, goal, trajectory)
                opt_trajectory = goals_probabilities.optimum_trajectory[goal_and_type]
                #7. and 8. Generate optimum trajectory from last observed point and smooth it
                current_trajectory = self._generate_trajectory(agentId, frame, goal, trajectory, maneuver)
                #10. join the observed and generated trajectories
                current_trajectory.insert(trajectory)
                goals_probabilities.current_trajectory[goal_and_type] = current_trajectory
                #6,9,10. calculate rewards, likelihood
                if self._reward_as_difference:
                    goals_probabilities.optimum_reward[goal_and_type] = None
                    goals_probabilities.current_reward[goal_and_type] = None
                    goals_probabilities.reward_difference[goal_and_type] = self._reward_difference(
                        opt_trajectory, current_trajectory, goal)
                else:
                    goals_probabilities.optimum_reward[goal_and_type] = self._reward(opt_trajectory, goal)
                    goals_probabilities.current_reward[goal_and_type] = self._reward(
                        current_trajectory, goal)
                    goals_probabilities.reward_difference[goal_and_type] = self._reward_difference(
                        opt_trajectory, current_trajectory, goal)
                likelihood = self._likelihood(opt_trajectory, current_trajectory, goal)
            except RuntimeError as e:
                logger.debug(str(e))
                likelihood = 0.
                goals_probabilities.current_trajectory[goal_and_type] = None
            #update goal probabilities
            goals_probabilities.goals_probabilities[goal_and_type] = goals_probabilities.goals_priors[
                goal_and_type] * likelihood
            goals_probabilities.likelihood[goal_and_type] = likelihood
            norm_factor += likelihood * goals_probabilities.goals_priors[goal_and_type]

        # then divide prob by norm_factor to normalise
        for key, prob in goals_probabilities.goals_probabilities.items():
            try:
                goals_probabilities.goals_probabilities[key] = prob / norm_factor
            except ZeroDivisionError as e:
                logger.debug("All goals unreacheable. Setting all probabilities to 0.")
                break

    def _generate_trajectory(self, agentId: int, frame: Dict[int, AgentState], goal: Goal, 
                state_trajectory: StateTrajectory, maneuver: Maneuver = None) -> VelocityTrajectory:
        """Generates a trajectory from the current frame of an agent to the specified goal"""
        trajectories, _ = self._astar.search(agentId, frame, goal, self._scenario_map, maneuver)
        if len(trajectories) == 0 : raise RuntimeError("Goal is unreachable")
        trajectory = trajectories[0]
        trajectory.velocity[0] = state_trajectory.velocity[-1]
        self._smoother.load_trajectory(trajectory)
        trajectory.velocity = self._smoother.split_smooth()
        return trajectory

    def _likelihood(self, optimum_trajectory : Trajectory, current_trajectory: Trajectory, goal: Goal) -> float :
        """Calculates the non normalised likelihood for a specified goal"""
        return np.clip(np.exp(self._beta * self._reward_difference(optimum_trajectory, current_trajectory, goal)), 1e-305, 1e305)

    def _reward(self, trajectory: Trajectory, goal: Goal) -> float:
        """Calculates the reward associated to a trajectory for a specified goal."""
        return - self._cost.trajectory_cost(trajectory, goal)

    def _reward_difference(self, optimum_trajectory : Trajectory, current_trajectory: Trajectory, goal: Goal):
        """If reward_as_difference is True, calculates the reward as a measure of similarity between the two 
        trajectories' attributes. Otherwise simply calculates the difference as the difference of the 
        individual rewards"""
        if self._reward_as_difference:
            return - self._cost.cost_difference_resampled(optimum_trajectory, current_trajectory, goal)
        else:
            return self._reward(current_trajectory, goal) - self._reward(optimum_trajectory, goal)
